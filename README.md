# ğŸ¤– gemma-3-4b-it GPU-Accelerated LLM Hosting

A production-ready solution for hosting Large Language Models (LLMs) with GPU acceleration on Azure Container Apps. This project demonstrates how to deploy Google's gemma-3-4b-it model downloaded from HuggingFace. It can also be adapted to host custom or fine-tuned models.

## ğŸ¯ Overview

This repository provides a complete stack for serving LLMs with:
- **FastAPI backend** with streaming response support
- **GPU-accelerated inference** using llama-cpp-python with CUDA
- **Modern web interface** with real-time streaming responses
- **Azure Container Apps deployment** with GPU workload profiles
- **Docker containerization** with NVIDIA CUDA support

## âœ¨ Key Features

- ğŸš€ **GPU Acceleration**: Utilizes NVIDIA GPUs for fast inference
- ğŸ“¦ **HuggingFace Integration**: Downloads models directly from HuggingFace Hub
- ğŸ”„ **Streaming Responses**: Real-time token-by-token response generation
- ğŸ¨ **Clean Web UI**: Markdown rendering with syntax highlighting
- â˜ï¸ **Azure Deployment**: Infrastructure as Code with Bicep templates
- ğŸ³ **Docker Ready**: Multi-stage build for optimized container images

## ğŸ§  Model Information

This project uses **Google gemma-3-4b-it Instruct** (quantized GGUF format) downloaded from HuggingFace:
- Model: `bartowski/gemma-3-4b-it-GGUF`
- Format: Quantized Q4_K_M GGUF for efficient inference
- Context: 8K tokens
- Size: ~2.7 GB

### ğŸ”§ Using Your Own Model

**This approach works with any GGUF model from HuggingFace or your own fine-tuned models!**

To use a different model:

1. **Update the Dockerfile** (line 30):
   ```dockerfile
   RUN hf download <your-org>/<your-model> <model-file.gguf> --local-dir .
   ```

2. **Update app.py** (line 37) with your model filename:
   ```python
   llm = Llama(
       model_path="./your-model-file.gguf",
       # ... other parameters
   )
   ```

3. **For private models**, pass your HuggingFace token during build:
   ```bash
   docker build --secret id=hf_token,src=hf_token.txt -t your-llm .
   ```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚
â”‚  (index.html)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server â”‚
â”‚    (app.py)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llama-cpp-pythonâ”‚
â”‚  (CUDA-enabled) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NVIDIA GPU    â”‚
â”‚  (A100/T4/etc)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

**Important:** Before downloading the gemma-3-4b-it model, you must:
1. Create a HuggingFace account at https://huggingface.co
2. Visit the model page: https://huggingface.co/google/gemma-3-4b-it
3. Accept the terms and conditions directly on the HuggingFace portal
4. Login to HuggingFace CLI: `hf auth login`

Without accepting the terms, you'll get access errors when attempting to download the model.

### Local Development (Without GPU)

```bash
# Install dependencies
pip install -r requirements.txt

# Login to HuggingFace (required for Gemma models)
hf auth login

# Download the model
hf download google/gemma-3-4b-it --local-dir .

# Run the server
uvicorn app:app --reload --host 0.0.0.0 --port 5000
```

Visit `http://localhost:5000/static/index.html`

### Docker Build (GPU-enabled)

```bash
# Build the image
docker build -t gemma-3-4b-gpu .

# Run with GPU support
docker run --gpus all -p 5000:5000 gemma-3-4b-gpu
```

## â˜ï¸ Azure Deployment

### Prerequisites

- Azure subscription
- Azure CLI installed and logged in
- Azure Container Registry (ACR)

### Deployment Steps

1. **Build and push Docker image**:
   ```powershell
   az acr build --registry <your-acr-name> --image gemma-3-4b-gpu:latest .
   ```

2. **Create GPU-enabled Container Apps environment**:
   ```powershell
   az containerapp env create `
     --name me-gpullm `
     --resource-group GPU.LLM.RG `
     --location eastus `
     --enable-workload-profiles
   
   az containerapp env workload-profile add `
     --name me-gpullm `
     --resource-group GPU.LLM.RG `
     --workload-profile-name NC24-A100 `
     --workload-profile-type NC24-A100
   ```

3. **Deploy the container app**:
   ```powershell
   az containerapp create `
     --name gemma-3-4b-gpu `
     --resource-group GPU.LLM.RG `
     --image <your-acr>.azurecr.io/gemma-3-4b-gpu:latest `
     --cpu 4 --memory 8Gi `
     --environment me-gpullm `
     --registry-server <your-acr>.azurecr.io `
     --ingress 'external' --target-port 5000 `
     --workload-profile-name NC24-A100 `
     --env-vars CUDA_VISIBLE_DEVICES=0 NVIDIA_VISIBLE_DEVICES=all `
     --min-replicas 1 --max-replicas 2 `
     --system-assigned
   ```

See `DEPLOYMENT.md` for detailed deployment instructions.

## ğŸ“ Project Structure

```
gemma-3-4b/
â”œâ”€â”€ app.py                  # FastAPI application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ dockerfile             # Multi-stage Docker build
â”œâ”€â”€ deployment.ps1         # Azure deployment script
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html         # Web interface
â”‚   â”œâ”€â”€ ollama.js          # Streaming response handler
â”‚   â”œâ”€â”€ style.css          # UI styling
â”‚   â””â”€â”€ showdown.min.js    # Markdown renderer
â””â”€â”€ infra/
    â”œâ”€â”€ main.bicep         # Azure infrastructure
    â”œâ”€â”€ main.json          # ARM template
    â””â”€â”€ main.parameters.json
```

## ğŸ”Œ API Endpoints

### `POST /predict`
Generate a complete response

### `POST /predictstream`
Generate streaming response (recommended)

### `GET /health`
Health check endpoint

### `GET /static/index.html`
Web interface

## ğŸ¨ Web Interface Features

- **Real-time streaming**: See responses as they're generated
- **Markdown rendering**: Properly formatted code blocks and text
- **Auto-clear input**: Question field clears after each response
- **Keyboard shortcut**: Press Enter to submit

## ğŸ” Environment Variables

- `CUDA_VISIBLE_DEVICES`: GPU device IDs to use
- `NVIDIA_VISIBLE_DEVICES`: NVIDIA device visibility
- `NVIDIA_DRIVER_CAPABILITIES`: Driver capabilities (compute, utility)

## ğŸ“Š Performance

- **GPU Layers**: All layers offloaded to GPU (`n_gpu_layers=-1`)
- **Batch Size**: Optimized for GPU (`n_batch=512`)
- **Memory Lock**: Model locked in memory for faster inference
- **Context Window**: 8192 tokens

## ğŸ› ï¸ Customization

### Adjusting Model Parameters

Edit `app.py` to modify:
- `n_ctx`: Context window size
- `n_gpu_layers`: Number of layers on GPU
- `n_batch`: Batch size for processing
- `chat_format`: Chat template format

### Styling the Web Interface

Modify `web/style.css` to customize the UI appearance.

## ğŸ“ License

This project is provided as-is for educational and development purposes.

## ğŸ¤ Contributing

Feel free to submit issues and enhancement requests!

## ğŸ“š Resources

- [Google Gemma 3 Models](https://huggingface.co/bartowski/gemma-3-4b-it-GGUF)
- [llama-cpp-python Documentation](https://llama-cpp-python.readthedocs.io/)
- [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/)
- [HuggingFace Hub](https://huggingface.co/docs/hub/index)

---

**Note**: This solution is perfect for hosting your own fine-tuned models! Simply replace the HuggingFace model download command with your custom model and deploy following the same process.
